Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 正态分布

正态分布，又叫做高斯分布，

若随机变量X，服从一个位置参数为μ、尺度参数为σ的概率分布，且其概率密度函数为：

$$
f(x)={1 \over \sqrt{2 \pi} \sigma} e^{- {(x-\mu_B)^2} \over 2\sigma^2} \tag{1}
$$

则这个随机变量就称为正态随机变量，正态随机变量服从的分布就称为正态分布，记作：

$$
X \sim N(\mu_B,\sigma^2) \tag{2}
$$

当μ=0,σ=1时，称为标准正态分布：

$$X \sim N(0,1) \tag{3}$$

此时公式简化为：

$$
f(x)={1 \over \sqrt{2 \pi}} e^{- {x^2} \over 2} \tag{4}
$$

下图就是三种（μ, σ）组合的函数图像：

<img src=".\Images\14\bn1.png">


# 深度神经网络的挑战

机器学习领域有个很重要的假设：IID独立同分布假设（独立同分布），就是假设训练数据和测试数据是满足相同分布的，这样就能做到通过训练数据获得的模型能够在测试集获得好的效果。

在深度神经网络中，我们可以将每一层视为对输入的信号做了一次变换：

$$
Z = W \cdot X + B \tag{5}
$$

经过这样的变换之后，神经网络的每一层所接受到的输入的分布就可能会发生变化，不再是输入的原始数据所适应的分布了。

比如，在上图中，假设X是服从蓝色曲线的分布，经过公式5后，有可能变成了绿色曲线的分布。

标准正态分布的数值密度占比如下图所示：

<img src=".\Images\14\bn2.png">

有68%的值落在[-1,1]之间，有95%的值落在[-2,2]之间。再回忆一下激活函数Sigmoid的图像，在[-2,2]之外的区域，其导数值很小（小于0.1）:
1. 激活后的值基本接近0或1了，饱和
2. 导数数值小，反向传播的力度很小

<img src=".\Images\8\sigmoid.png">

在前向计算过程中，如果数据整体真的向右偏移了两个单位，在下图中可以看到，在[2,4]范围内的数据（蓝色），都处于激活函数的饱和区（黄色），对于训练就很不利了。

<img src=".\Images\14\bn3.png">

有的人会问，我们在深度学习中不是都用ReLU激活函数吗？那么BN对于ReLU有用吗？下面我们看看ReLU函数的图像：

<img src=".\Images\14\bn4.png">

上图中蓝色为数据分布，黄色为ReLU的激活值，可以看到几乎所有的数据都被前传到了下一层网络中，而没有被小于0的部分剪裁，那么这个网络和线性网络也差不多了，失去了深层网络的能力。

# 批归一化 Batch Normalization

有的书翻译成归一化，有的翻译成正则化，英文Batch Normalization，简称为BatchNorm，或BN。

在上一小节中，我们学习了权重值的初始化，是发生在神经网络的第一层。如果在深度神经网络的每一层，都可以有类似的手段，那么训练效果就会很理想。这就是批归一化的想法的来源。

1. 随着网络的加深，在深层的层接收到的输入的分布和输入的分布可能差距很大，这会在一定程度上影响神经网络I.I.D.（独立同分布）的假设

2. 随着网络的加深，深层的层接受到的数据分布会因为每次训练导致的参数变化而不断变化，那么这些层就既需要去提取特征，又需要去适应不断变化的分布，加大了训练和收敛的难度

深度神经网络随着网络深度加深，训练起来越困难，收敛越来越慢，这是个在DL领域很接近本质的问题。很多论文都是解决这个问题的，比如ReLU激活函数，再比如Residual Network，BN本质上也是解释并从某个不同的角度来解决这个问题的。

BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的，致力于将每一层的输入数据正则化成$N(0,1)$的分布。

为了不影响网络本身的表达能力，以及可能一些层对非正态分布的数据表现更为优异，给予网络一定的调整能力，在正则化后，额外使用两个参数$\gamma$，$\beta$对分布进行了一定的线性变化。

在实际的工程中，我们把BN当作一个层来看待。

# 前向计算

## 符号表

|符号|数据类型|数据形状|
|:---------:|:-----------:|:---------:|
|$X$| 输入数据矩阵 | [batchSize, Features] |
|$x_i$|输入数据第i个样本| [1, Features] |
|$N$| 经过归一化的数据矩阵 | [batchSize, Features] |
|$n_i$| 经过归一化的单样本 | [1, Features] |
|$\mu_B$| 批数据均值 | [1, Features] |
|$\sigma^2_B$| 批数据方差 | [1, Features] |
|$m$|批样本数量| [1] |
|$\gamma$|线性变换参数| [1, Features] |
|$\beta$|线性变换参数| [1, Features] |
|$Z$|线性变换后的矩阵| [1, Features] |
|$z_i$|线性变换后的单样本| [1, Features] |
|$\delta$| 反向传入的误差 | [batchSize, Features] |

P.S. 无特殊说明，以下乘法为元素乘，即element wise的乘法

在训练过程中，针对每一个batch数据，m是批的大小。进行的操作是，将这组数据正则化，之后对其进行线性变换。

具体的算法步骤是：

$$
\mu_B = {1 \over m}\sum_1^m x_i \tag{6}
$$

$$
\sigma^2_B = {1 \over m} \sum_1^m (x_i-\mu_B)^2 \tag{7}
$$

$$
n_i = {x_i-\mu_B \over \sqrt{\sigma^2_B + \epsilon}} \tag{8}
$$

$$
z_i = \gamma n_i + \beta \tag{9}
$$

其中，$\gamma 和 \beta$是训练出来的，$\epsilon$是防止$\mu_B^2$为0时加的一个很小的数值，通常为1e-5。

# 计算图（示意）

下图是一张示意的计算图，用于帮助我们搞清楚正向和反向的过程：

<img src=".\Images\14\bn5.png">

$X1,X2,X3$表示三个样本（实际上一般用32，64这样的批大小），我们假设每个样本只有一个特征值（否则X将会是一个样本数乘以特征值数量的矩阵）。

# 反向传播

首先假设已知从上一层回传给BN层的误差矩阵是：

$$\delta = {dJ \over dZ}，\delta_i = {dJ \over dz_i} \tag{10}$$

则根据公式9，求$\gamma 和 \beta$的梯度：

$${dJ \over d\gamma} = \sum_{i=1}^m {dJ \over dz_i}{dz_i \over d\gamma}=\sum_{i=1}^m \delta_i \cdot n_i \tag{11}$$

$${dJ \over d\beta} = \sum_{i=1}^m {dJ \over dz_i}{dz_i \over d\beta}=\sum_{i=1}^m \delta_i \tag{12}$$

注意$\gamma和\beta$的形状与批大小无关，只与特征值数量有关，我们假设特征值数量为1，所以它们都是一个标量，因此会用求和方式计算。


如果已知BN层的输入误差L，想要求BN层的输出误差，以便回传到前层。即求z_i对x_i的偏导。

从正向公式中看：
- $z_i \leftarrow n_i \leftarrow x_i$
- $z_i \leftarrow n_i \leftarrow \mu_B \leftarrow x_i$
- $z_i \leftarrow n_i \leftarrow \sigma^2_B \leftarrow x_i$
- $z_i \leftarrow n_i \leftarrow \sigma^2_B \leftarrow \mu_B \leftarrow x_i$


从公式8，9：

$$
{dJ \over dx_i} = {dJ \over d n_i}{d n_i \over dx_i} + {dJ \over d \sigma^2_B}{d \sigma^2_B \over dx_i} + {dJ \over d \mu_B}{d \mu_B \over dx_i} \tag{13}
$$

公式13的右侧第一部分：

$$
{dJ \over d n_i}=  {dJ \over dz_i}{dz_i \over dn_i} = \delta_i \cdot \gamma\tag{14}
$$

公式13的右侧第二部分，从公式8：
$$
{d n_i \over dx_i}={1 \over \sqrt{\sigma^2_B + \epsilon}} \tag{15}
$$

公式13的右侧第三部分，从公式8：
$$
{dJ \over d \sigma^2_B}=\sum_{i=1}^m {dJ \over d n_i}{d n_i \over d \sigma^2_B}
$$
$$
=-{1 \over 2}\sum_{i=1}^m {dJ \over d n_i} \cdot (x_i-\mu_B)(\sigma^2_B + \epsilon)^{-3/2} \tag{16}
$$

$$
=\delta \cdot \gamma \cdot  {-1 \over 2}(X-\mu_B)(\sigma^2_B + \epsilon)^{-3/2}
$$

公式13的右侧第四部分，从公式7：
$$
{d \sigma^2_B \over dx_i} = {2(x_i - \mu_B) \over m} \tag{17}
$$

公式13的右侧第五部分，从公式7，8：

$$
{dJ \over d \mu_B}={dJ \over d n_i}{d n_i \over d \mu_B} + {dJ \over  d\sigma^2_B}{d \sigma^2_B \over d \mu_B} \tag{18}
$$

公式18的右侧第二部分，根据公式8：

$$
{d n_i \over d \mu_B}={-1 \over \sqrt{\sigma^2_B + \epsilon}} \tag{19}
$$

公式18的右侧第四部分，根据公式7：

$$
{d \sigma^2_B \over d \mu_B}={-2 \over m}\sum_{i=1}^m (x_i- \mu_B) ={-2 \over m}(X-\mu_B) \tag{20}
$$

所以公式18是：

$$
{dJ \over d \mu_B}=\delta \cdot \gamma \cdot {-1 \over \sqrt{\sigma^2_B + \epsilon}} + {dJ \over d \sigma^2_B}{-2 \over m} (X- \mu_B) \tag{21}
$$

公式13的右侧第六部分，从公式6：

$$
{d \mu_B \over dx_i} = {1 \over m} \tag{22}
$$

所以，公式13最后是这样的：

$$
{dJ \over dx_i} = \delta \cdot \gamma \cdot {1 \over \sqrt{\sigma^2_B + \epsilon}} + {dJ \over d\sigma^2_B} \cdot {2(x_i - \mu_B) \over m} + {dJ \over d\mu_B} \cdot {1 \over m} \tag{13}
$$

# 测试和推理时的归一化方法

在测试过程或推理时，我们只有一张或者几张图的数据，无法算出来正确的均值，因此，我们使用的均值和方差数据是在测试过程中样本值的平均。也就是：

$$
E[x] = E[\mu_B]
$$
$$
Var[x] = {m \over m-1} E[\sigma^2_B]
$$



如上所述，我们给出前向传播的代码

```python
class batchNorm():
    """Batch normalization layer implemented by numpy

    This is the basic class for batch normalization layer used in deep learning

    Attributes:
        input: the name of layer before this layer
        output: the name of layer after this layer
        mean_list: the list stores the mean values of variables trained using this layer
        var_list: the list stores the variance values of variables trained using this layer
        eps: a small value to avoid the variance is zero
        gamma: a learnable gamma to do linear transformation
        beta: a learnable beta to do linear transformation
        variable: store the input variable
        gradient_gamma: the gradient to update gamma
        gradient_beta: the gradient to update beta
    """

    def __init__(self, input, output, outputChannel):
        """Construct a BN layer"""
        self.input = input
        self.output = output
        self.mean_list = []
        self.var_list = []
        self.eps = 1e-5
        self.gamma = np.ones([1, outputChannel])
        self.beta = np.zeros([1, outputChannel])

    def forward(self, variable, train=True):
        """the function used in forward process

        This function describes the process of forward calculation

        Args:
            variable: the input variable to be processed, 
                      assume the shape of this variable is [batch, Features]
            train: a bool variable represents whether it is still under training 

        Return:
            the variable after processed, [batch, Features]

        """
        if train:
            mean = np.mean(variable, axis=0)
            var = np.var(variable, axis=0)
            self.variable = variable
            variable = (variable - mean) / np.sqrt(var + self.eps)
            self.mean_list.append(mean)
            self.var_list.append(var)
        else:
            mean = sum(self.mean_list) / len(self.mean_list)
            var = sum(self.var_list) / len(self.var_list)
            var = var * self.variable.shape[0] / (self.variable.shape[0] - 1)
            variable = (variable - mean) / np.sqrt(var + self.eps)
        # end if

        return self.gamma * variable + self.beta
```

# 反向过程

在BN层的反向传播过程中，我们需要计算哪些东西呢？
首先是两个可学习的变量$\gamma$,$\beta$的梯度，我们需要计算出对应的梯度信息来更新这两个变量，其次，我们需要计算出这一层的误差函数来继续进行传播。计算梯度信息的过程和全连接层计算的过程几乎完全一样，这里只给出计算公式：

$$\gamma_{gradient}=C\times \hat{X} = \sum_i c_ix_i$$
$$\beta_{gradient} = \sum_i c_i$$

下面我们来推导误差传递的代码，根据反向传播公式，我们需要计算的是
$\frac{\partial C}{\partial X}$，根据链式法则，我们先计算$\frac{\partial C}{\partial \hat{X}}$。根据和全连接层类似的推导，我们可以很容易的得到：

$$
\frac{\partial C}{\partial \hat{X}} = \gamma C
$$

下面，根据链式法则，

$$
\frac{\partial C}{\partial X} = \frac{\partial C}{\partial \hat{X}} \frac{\partial \hat{X}}{\partial X} + \frac{\partial C}{\partial \sigma ^2} \frac{\partial \sigma^2}{\partial X} + \frac{\partial C}{\partial \mu_B} \frac{\partial \mu_B}{\partial X}
$$

我们分别来看各个部分的导数，

首先是，此处$\gamma C$ 为元素之间相乘的结果，因为各Channel的数据对应各自的常数$\gamma_i$

$$\frac{\partial C}{\partial \hat{X}} \frac{\partial \hat{X}}{\partial X} = \gamma C \frac{1}{\sqrt{\sigma^2 + \varepsilon}}$$

接着是

$$\frac{\partial C}{\partial \sigma ^2} \frac{\partial \sigma^2}{\partial X} = \frac{\partial C}{\partial \hat{X}} \frac{\partial \hat{X}}{\partial \sigma ^2} \frac{\partial \sigma^2}{\partial X}$$
$$ = \gamma C \cdot (X - \mu_B) \cdot (\frac{-0.5}{{(\sigma^2 + \varepsilon)}^{\frac{3}{2}}}) \frac{1}{m}$$
$$=(\sum_i (\gamma_i C_i) (x_i - \mu_B)) (\frac{-0.5}{{(\sigma^2 + \varepsilon)}^{\frac{3}{2}}})\frac{1}{m}$$

最后来看：

$$
\frac{\partial C}{\partial \mu_B} \frac{\partial \mu_B}{\partial X} = (\frac{\partial C}{\partial \hat{X}} \frac{\partial \hat{X}}{\partial \mu_B} + \frac{\partial C}{\partial \sigma^2} \frac{\partial \sigma^2}{\partial \mu_B})\frac{\partial \mu_B}{\partial X}$$
$$ = (\sum_i^m \gamma_i C_i \frac{-1}{\sqrt{\sigma^2 + \varepsilon}} + \frac{\partial C}{\partial \sigma^2} \cdot (-2)(X - \mu_B) \frac{1}{m}) \cdot \frac{1}{m}
$$

因此，我们只需要根据链式法则将上述几个式子相加即可得到最终结果。用代码表述即为：

```python
def backward(self, error):
        """backward function used in training

        This function will compute the gradient of the gamma and beta in BN layer
        and return error to previous layer

        Args:
            error: the error returned by the later layer in the network

        Return:
            the computed error to transfer to the previous layer
        """

        batch_size = self.variable.shape[0]
        mean = self.mean_list[-1]
        var = self.var_list[-1]

        # calculate the gradient to update gamma and beta
        self.gradient_gamma = np.sum(error * ((self.variable - mean) / np.sqrt(var + self.eps)), axis=0)
        self.gradient_beta = np.sum(error, axis=0)

        # the error to normalized variable, [1, Features] .* [batch, Features] = [batch, Features]
        error_normalized_variable = self.gamma * error

        # the error related to the var of the input variable, [1, Features]
        error_var = np.sum(error_normalized_variable * (self.variable - mean) * (-0.5) * (var + self.eps) ** (-1.5), axis = 0)

        # the error related to the mean of the input variable, [1, Features]
        error_mean = (-1) * np.sum(error_normalized_variable / np.sqrt(var + self.eps), axis=0) + error_var * (-2) * np.mean(self.variable - mean, axis=0)

        # sum the above error items to get the final error
        error = error_normalized_variable / np.sqrt(var + self.eps) + (error_var * 2 * (self.variable - mean) + error_mean) / batch_size

        return error
```

- 没有它之前，需要小心的调整学习率和权重初始化，但是有了BN可以放心的使用大学习率，但是使用了BN，就不用小心的调参了，较大的学习率极大的提高了学习速度，
- Batchnorm本身上也是一种正则的方式，可以代替其他正则方式如dropout等

# 参考

https://arxiv.org/abs/1502.03167